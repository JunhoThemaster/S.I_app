import React, { useEffect, useRef, useState } from "react";
import { FaceMesh } from "@mediapipe/face_mesh";
import { Pose } from "@mediapipe/pose";
import { Camera } from "@mediapipe/camera_utils";
import {
  drawCustomFaceMesh,
  drawPoseSkeleton,
} from "../utils/drawingUtils";

interface PoseTrackerProps {
  onCameraReady: () => void;
}

const PoseTracker: React.FC<PoseTrackerProps> = ({ onCameraReady }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const cameraRef = useRef<Camera | null>(null);
  const socketRef = useRef<WebSocket | null>(null);
  const hasStartedRef = useRef(false);

  const [blinkCount, setBlinkCount] = useState(0);
  const blinkCountRef = useRef(0);
  const [analysisResult, setAnalysisResult] = useState("ë¶„ì„ ëŒ€ê¸° ì¤‘");
  const postureRef = useRef("ì •ìƒ");
  const [postureState, setPostureState] = useState("ì •ìƒ");
  const unstableSideRef = useRef<"left" | "right" | null>(null); // âœ… ì¦‰ì‹œ ë Œë”ë§ì„ ìœ„í•œ ref
  const earRef = useRef(0);

  const faceResultsRef = useRef<any>(null);
  const poseResultsRef = useRef<any>(null);
  const wasEyeClosedRef = useRef(false);

  const EAR_THRESHOLD = 0.21;

  const calculateEAR = (landmarks: any): number => {
    const left = [33, 160, 158, 133, 153, 144];
    const right = [362, 385, 387, 263, 373, 380];
    const dist = (a: any, b: any) => Math.hypot(a.x - b.x, a.y - b.y);
    const avgEAR = (eye: number[]) => {
      const lm = eye.map((i) => landmarks[i]);
      return ((dist(lm[1], lm[5]) + dist(lm[2], lm[4])) / (2 * dist(lm[0], lm[3])));
    };
    return (avgEAR(left) + avgEAR(right)) / 2;
  };

  const processBlink = (ear: number) => {
    if (ear < EAR_THRESHOLD && !wasEyeClosedRef.current) {
      blinkCountRef.current += 1;
      setBlinkCount((prev) => prev + 1);
      wasEyeClosedRef.current = true;
      console.log("ğŸ‘ï¸ ëˆˆ ê¹œë¹¡ì„ ê°ì§€");
    } else if (ear >= EAR_THRESHOLD) {
      wasEyeClosedRef.current = false;
    }
  };

  const judgePostureFromPose = (poseLandmarks: any) => {
    if (!poseLandmarks) return;

    const left = poseLandmarks[11];
    const right = poseLandmarks[12];
    const dy = left.y - right.y;
    const threshold = 0.06;

    if (Math.abs(dy) > threshold) {
      const direction: "left" | "right" = dy > 0 ? "left" : "right";
      postureRef.current = "ë¶ˆì•ˆì •";
      setPostureState("ë¶ˆì•ˆì •");
      unstableSideRef.current = direction; // âœ… ë°©í–¥ ì¦‰ì‹œ ì €ì¥
      console.log("ğŸ“ ìì„¸ ìƒíƒœ: ë¶ˆì•ˆì • | â†” ë°©í–¥:", direction);
    } else {
      postureRef.current = "ì •ìƒ";
      setPostureState("ì •ìƒ");
      unstableSideRef.current = null;
      console.log("ğŸ“ ìì„¸ ìƒíƒœ: ì •ìƒ");
    }
  };

  const sendFeaturesWithImage = () => {
    const video = videoRef.current;
    if (!video || socketRef.current?.readyState !== WebSocket.OPEN) return;

    const faceLandmarks = faceResultsRef.current?.multiFaceLandmarks?.[0];
    const poseLandmarks = poseResultsRef.current?.poseLandmarks;

    if (!faceLandmarks) {
      console.warn("âš ï¸ ì–¼êµ´ ëœë“œë§ˆí¬ ì—†ìŒ");
      return;
    }

    const irisX = (faceLandmarks[468]?.x + faceLandmarks[473]?.x) / 2 || 0;
    const irisY = (faceLandmarks[468]?.y + faceLandmarks[473]?.y) / 2 || 0;
    const nose = faceLandmarks[1];
    const headPose = nose ? [nose.x, nose.y, nose.z] : [0, 0, 0];

    if (poseLandmarks) {
      judgePostureFromPose(poseLandmarks);
    }

    const canvas = document.createElement("canvas");
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const base64 = canvas.toDataURL("image/jpeg").split(",")[1];

    const payload = {
      timestamp: new Date().toISOString(),
      blink_count: blinkCountRef.current,
      gaze_x: irisX,
      gaze_y: irisY,
      head_pose: headPose,
      posture: postureRef.current,
      image: base64,
      ear: earRef.current,
    };

    socketRef.current.send(JSON.stringify(payload));
    console.log("ğŸ“¤ ì„œë²„ ì „ì†¡:", payload);

    blinkCountRef.current = 0;
    setBlinkCount(0);
  };

  const drawAll = () => {
    const canvas = canvasRef.current;
    const ctx = canvas?.getContext("2d");
    if (!ctx || !canvas) return;

    ctx.save();
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.translate(canvas.width, 0);
    ctx.scale(-1, 1);

    const video = videoRef.current;
    if (video) {
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    }

    if (faceResultsRef.current?.multiFaceLandmarks) {
      for (const landmarks of faceResultsRef.current.multiFaceLandmarks) {
        drawCustomFaceMesh(ctx, landmarks);
      }
    }

    if (poseResultsRef.current?.poseLandmarks) {
      drawPoseSkeleton(ctx, poseResultsRef.current.poseLandmarks);
    }

    ctx.restore();
  };

  useEffect(() => {
    if (hasStartedRef.current) return;
    hasStartedRef.current = true;

    const videoElement = videoRef.current!;
    let faceMesh: FaceMesh;
    let pose: Pose;

    const initMediaPipe = () => {
      faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });
      faceMesh.onResults((results) => {
        faceResultsRef.current = results;
        const landmarks = results?.multiFaceLandmarks?.[0];
        if (landmarks) {
          const ear = calculateEAR(landmarks);
          earRef.current = ear;
          processBlink(ear);
          //console.log("ğŸ‘ï¸ EAR:", ear.toFixed(3));
        }
        drawAll();
      });

      pose = new Pose({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`,
      });
      pose.setOptions({
        modelComplexity: 1,
        smoothLandmarks: true,
        enableSegmentation: false,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });
      pose.onResults((results) => {
        poseResultsRef.current = results;
        drawAll();
      });
    };

    const initWebSocket = () => {
      socketRef.current = new WebSocket("ws://localhost:8000/ws/video");

      socketRef.current.onopen = () => {
        console.log("âœ… WebSocket ì—°ê²°ë¨");
        setInterval(() => {
          sendFeaturesWithImage();
        }, 1000);
      };

      socketRef.current.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log("ğŸ“¨ ê°ì • ê²°ê³¼:", data.emotion);
          console.log("ğŸ‘ï¸ ëˆˆ ê¹œë¹¡ì„ ìˆ˜:", data.blink_count);
          console.log("ğŸ“ ìì„¸ ìƒíƒœ:", data.posture);
          setAnalysisResult(data.emotion);
        } catch (err) {
          console.warn("âš ï¸ WebSocket ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", err);
        }
      };

      socketRef.current.onerror = (err) => {
        console.error("âŒ WebSocket ì—ëŸ¬:", err);
      };
    };

    const startCamera = () => {
      cameraRef.current = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
          await pose.send({ image: videoElement });
        },
        width: 640,
        height: 480,
      });
      cameraRef.current.start().then(() => {
        console.log("ğŸ“¸ ì¹´ë©”ë¼ ì‹œì‘ë¨");
      });
    };

    videoElement.onloadeddata = () => {
      console.log("ğŸ¥ ì¹´ë©”ë¼ ì¬ìƒë¨ â†’ WebSocket ì‹œì‘");
      onCameraReady();
      initWebSocket();
    };

    initMediaPipe();
    startCamera();

    return () => {
      cameraRef.current?.stop();
      socketRef.current?.close();
    };
  }, [onCameraReady]);

  return (
    <div className="camera-container">
      <video
        ref={videoRef}
        className="video-feed"
        playsInline
        muted
        autoPlay
        width={640}
        height={480}
        style={{ position: "absolute", left: 0, top: 0, zIndex: 0 }}
      />
      <canvas
        ref={canvasRef}
        className="canvas-overlay"
        width={640}
        height={480}
      />

      {/* âœ… ë°©í–¥ ë”°ë¼ ë°˜ì§ì´ UI ì¡°ê±´ë¶€ í‘œì‹œ */}
     {postureState === "ë¶ˆì•ˆì •" && (
  <div className="blinker-warning">
    <div className="blinker-slot">
      {unstableSideRef.current === "left" && <div className="blinker left" />}
    </div>
    <p className="warning-text">ìì„¸ê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤!</p>
    <div className="blinker-slot">
      {unstableSideRef.current === "right" && <div className="blinker right" />}
    </div>
  </div>
)}

      <p style={{ marginTop: "500px", fontSize: "18px", fontWeight: "bold" }}>
        í˜„ì¬ ê°ì • ìƒíƒœ: {analysisResult}
      </p>
    </div>
  );
};

export default PoseTracker;
