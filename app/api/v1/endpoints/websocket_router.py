# from fastapi import APIRouter, WebSocket
# from app.services.analyzer import analyze_vector, emotion_to_onehot
# from PIL import Image
# import io
# import json
# import base64
# import numpy as np
# from deepface import DeepFace

# router = APIRouter()

# @router.websocket("/ws/analyze")
# async def analyze_ws(websocket: WebSocket):
#     await websocket.accept()
#     print("ğŸŸ¢ WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")

#     total_blinks = 0  # âœ… ì „ì²´ ëˆ„ì  ê¹œë¹¡ì„ ìˆ˜

#     while True:
#         try:
#             print("ğŸ“¥ ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
#             data_raw = await websocket.receive_text()
#             data = json.loads(data_raw)
#             print("ğŸ“© ìˆ˜ì‹ ëœ JSON keys:", list(data.keys()))

#             # âœ… ì´ë¯¸ì§€ ë””ì½”ë”©
#             image_data = base64.b64decode(data["image"])
#             image = Image.open(io.BytesIO(image_data)).convert("RGB")
#             np_img = np.array(image)

#             # âœ… DeepFace ê°ì • ë¶„ì„
#             try:
#                 result = DeepFace.analyze(np_img, actions=["emotion"], enforce_detection=False)[0]
#                 emotion = result["dominant_emotion"]
#                 confidence = result["emotion"][emotion] / 100
#                 print(f"ğŸ§  ê°ì • ë¶„ì„: {emotion} ({confidence:.2f})")
#             except Exception as e:
#                 print("âŒ DeepFace ë¶„ì„ ì‹¤íŒ¨:", str(e))
#                 await websocket.send_json({"error": "emotion_analysis_failed"})
#                 continue

#             # âœ… MLP ì…ë ¥ ë²¡í„° êµ¬ì„±
#             vector = (
#                 emotion_to_onehot(emotion) +
#                 [confidence, data["gaze_x"], data["gaze_y"], data["ear"], data["blink_count"]] +
#                 data["head_pose"]
#             )

#             prediction = analyze_vector(vector)
#             print("âœ… ê°ì • ì˜ˆì¸¡ ê²°ê³¼:", prediction)

#             # âœ… ê¹œë¹¡ì„ ëˆ„ì  ê³„ì‚°
#             blink_delta = data["blink_count"]
#             total_blinks += blink_delta
#             print(f"ğŸ‘ï¸ ì´ë²ˆ ê¹œë¹¡ì„ ìˆ˜: {blink_delta}, ëˆ„ì : {total_blinks}")

#             # âœ… ì‘ë‹µ êµ¬ì„±
#             response = {
#                 "emotion": prediction,
#                 "raw_emotion": emotion,
#                 "confidence": round(confidence, 3),
#                 "blink_count": blink_delta,
#                 "total_blink_count": total_blinks,
#                 "posture": data["posture"]
#             }

#             await websocket.send_json(response)
#             print("ğŸ“¤ ì‘ë‹µ ì „ì†¡ ì™„ë£Œ:", response)

#         except Exception as e:
#             print("âŒ WebSocket ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", str(e))
#             break
from fastapi import APIRouter, WebSocket
from app.services.analyzer import analyze_vector, emotion_to_onehot
from PIL import Image
import io
import json
import base64
import numpy as np
from deepface import DeepFace

router = APIRouter()

@router.websocket("/ws/analyze")
async def analyze_ws(websocket: WebSocket):
    await websocket.accept()
    print("ğŸŸ¢ WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")

    total_blinks = 0  # âœ… ì „ì²´ ëˆ„ì  ê¹œë¹¡ì„ ìˆ˜

    while True:
        try:
            print("ğŸ“¥ ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
            data_raw = await websocket.receive_text()
            data = json.loads(data_raw)
            print("ğŸ“© ìˆ˜ì‹ ëœ JSON keys:", list(data.keys()))

            # âœ… ì´ë¯¸ì§€ ë””ì½”ë”©
            image_data = base64.b64decode(data.get("image", ""))
            image = Image.open(io.BytesIO(image_data)).convert("RGB")
            np_img = np.array(image)

            # âœ… DeepFace ê°ì • ë¶„ì„
            try:
                result = DeepFace.analyze(np_img, actions=["emotion"], enforce_detection=False)[0]
                emotion = result["dominant_emotion"]
                confidence = result["emotion"][emotion] / 100
                print(f"ğŸ§  ê°ì • ë¶„ì„: {emotion} ({confidence:.2f})")
            except Exception as e:
                print("âŒ DeepFace ë¶„ì„ ì‹¤íŒ¨:", str(e))
                await websocket.send_json({"error": "emotion_analysis_failed"})
                continue

            # âœ… ë°©ì–´ ì²˜ë¦¬ëœ ì…ë ¥ê°’ ì¶”ì¶œ
            gaze_x = data.get("gaze_x", 0.0)
            gaze_y = data.get("gaze_y", 0.0)
            ear = data.get("ear", 0.0)  # â† ì—ëŸ¬ ë°©ì§€ í•µì‹¬
            blink_count = data.get("blink_count", 0)
            head_pose = data.get("head_pose", [0.0, 0.0, 0.0])
            posture = data.get("posture", 0)

            print(f"ğŸ‘ï¸ EAR: {ear} | ğŸ‘€ Gaze: ({gaze_x}, {gaze_y}) | ğŸ§  Head Pose: {head_pose}")

            # âœ… MLP ì…ë ¥ ë²¡í„° êµ¬ì„±
            vector = (
                emotion_to_onehot(emotion) +
                [confidence, gaze_x, gaze_y, ear, blink_count] +
                head_pose
            )

            prediction = analyze_vector(vector)
            print("âœ… ê°ì • ì˜ˆì¸¡ ê²°ê³¼:", prediction)

            # âœ… ê¹œë¹¡ì„ ëˆ„ì  ê³„ì‚°
            blink_delta = blink_count
            total_blinks += blink_delta
            print(f"ğŸ‘ï¸ ì´ë²ˆ ê¹œë¹¡ì„ ìˆ˜: {blink_delta}, ëˆ„ì : {total_blinks}")

            # âœ… ì‘ë‹µ êµ¬ì„±
            response = {
                "emotion": prediction,
                "raw_emotion": emotion,
                "confidence": round(confidence, 3),
                "blink_count": blink_delta,
                "total_blink_count": total_blinks,
                "posture": posture
            }

            await websocket.send_json(response)
            print("ğŸ“¤ ì‘ë‹µ ì „ì†¡ ì™„ë£Œ:", response)

        except Exception as e:
            print("âŒ WebSocket ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", str(e))
            break
