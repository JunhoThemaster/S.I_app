import numpy as np
import librosa
import sounddevice as sd
import torch
from services.audio_module import extract_melspectogram
from DL_model import CNNBiLSTM

# π“ μ¤λ””μ¤ νλΌλ―Έν„° μ„¤μ •
SR = 16000           # μƒν”λ§ λ μ΄νΈ
DURATION = 4         # λ…Ήμ μ‹κ°„(μ΄)
SAMPLES = SR * DURATION
DEVICE_INDEX = 1 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNBiLSTM().to(device)
model.load_state_dict(torch.load("audio_model.pth", map_location=device))
model.eval()
label_classes = np.load("label_encoder_classes.npy", allow_pickle=True)




def predict_emotion(audio):
    mel_tensor = extract_melspectogram.preprocess_audio(audio).to(device)

    # μ‹κ°ν™”
    mel_np = mel_tensor.squeeze().cpu().numpy()
  

    # μ¶”λ΅ 
    with torch.no_grad():
        output = model(mel_tensor)
        pred = output.argmax(1).item()
        probs = torch.softmax(output, dim=1).cpu().numpy().squeeze()

    print("π” κ°μ •λ³„ ν™•λ¥ :")
    for i, p in enumerate(probs):
        print(f"  {label_classes[i]:<10}: {p:.2f}")

    return label_classes[pred]
